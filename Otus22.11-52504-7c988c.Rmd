---
title: "Otus_R_ML algorithms"
date: '21 ноября 2019 г '
output: html_document

---
![](https://i.paste.pics/6b0eeb7945cbeeb850ae6a32dadee547.png)

![](https://i.paste.pics/40e1753a9259b836ffb3b496a092683a.png)

**Naive Bayes - k-means - random forest - logit **


### NAIVE BAYES - ТЕОРИЯ И ПРАКТИКА

**Что такое Наивный Байес**

Наивный байесовский метод является одним из самых простых в реализации алгоритмов классификации. Учитывая некоторый набор возможных классов (например, возрастные цензы для кино -  ребенок, подросток, взрослый или G, PG, PG-13, R), вы используете шаблоны внутри этих классов для маркировки новых, неклассифицированных данных. Самым большим его преимуществом является то, что вам действительно нужно только подсчитать, как часто значения каждой переменной встречаются для каждого класса. Таким образом, если у вас есть пять переменных, усредняющих три разных значения с двумя возможными классами, то это 5 x 3 x 2 = 30 частей информации, которые вам нужно сохранить. Вам также понадобится «предыдущая вероятность» для каждого класса. Проще говоря, вы подсчитываете все экземпляры каждого класса и делите на сумму всех экземпляров. Наивный Байес использует теорему Байеса, но предполагает, что все переменные в модели не зависят друг от друга. Это удобно, потому что позволяет вам умножать все вместе без необходимости вычислять более сложные условные вероятности, которые потребуются в теории вероятностей.
 
**Расчет и объединение вероятностей**

Если вы можете делить, вы можете использовать Наивный Байес;)

|     Class       |     Visited Other Dept     |     Did Not Visit    Other Dept     |     Came Alone     |     Did Not Come Alone     |     Total     |
|-----------------|----------------------------|-------------------------------------|--------------------|----------------------------|---------------|
|    Browser      |    200                     |    0                                |    75              |    125                     |    200        |
|    Easy Sale    |    150                     |    200                              |    250             |    100                     |    350        |
|    Big Sale     |    60                      |    60                               |    0               |    120                     |    120        |


• Какова предыдущая (то есть общая) вероятность того, что клиент будет зашел просто посмотреть? 
200 / (200 + 350 + 120) = 200 / 670 = 29.8507%

•	Вероятность большой покупки:
120 / (200 + 350 + 120) = 120 / 670 = 17.9104%

•	Вероятность посещения других отделов, если клиент совершил маленькую покупку:
P(Visited Other | Easy Sale)  = 150 / (150 + 200) = 150 / 350 = 42.8571%

• Какова вероятность того, что клиент пришел не один, учитывая, что он является клиентом, который совершил большую покупку?
P(Did Not Come Alone | Big Sale) = 120 / (0 + 120) = 100%

Суть в том, что нам нужно вычислить все эти «условные вероятности» вместе с предыдущими вероятностями для каждого класса. 

Вот результаты для примера таблицы.

| CLASS     | VISITED OTHER DEPT | DID NOT VISIT OTHER DEPT | CAME ALONE | DID NOT COME ALONE | PRIORS  |
|-----------|--------------------|--------------------------|------------|--------------------|---------|
| Browser   | 100.000%           | 0.000%                   | 37.500%    | 62.500%            | 29.851% |
| Easy Sale | 42.857%            | 57.143%                  | 71.429%    | 28.571%            | 52.239% |
| Big Sale  | 50.000%            | 50.000%                  | 0.000%     | 100.000%           | 17.910% |

Используя эти результаты, мы можем теперь угадать / классифицировать новые данные. Допустим, вы хотите классифицировать клиента, которого вы заметили. Он пришел один и посетил другой отдел. Теперь мы выстраиваем все необходимые вероятности и умножаем их вместе. Это сделано из-за наивного предположения, что каждая переменная не зависит друг от друга.

| Are They… | VISITED OTHER DEPT | CAME ALONE | PRIORS  |
|-----------|--------------------|------------|---------|
| Browser   | 100%               | 37.500%    | 29.851% |
| Easy Sale | 42.857%            | 71.429%    | 52.239% |
| Big Sale  | 50%                | 0%         | 17.910% |

Results for Browser: 0.111941
Results for Easy Sale: 0.159916
Results for Big Sale: 0.000000

Лучший результат - маленькая покупка. Если вы хотите получить фактическую вероятность для каждого класса, вы должны использовать результат в качестве числителя и сумму всех результатов в качестве знаменателя.

Probability for Browser = 0.111941 / (0.111941 + 0.159916 + 0.0) = 41.2%
Probability for Easy Sale= 0.159916 / (0.111941 + 0.159916 + 0.0) = 58.8%
Probability for Big Sale = 0.000000/ (0.111941 + 0.159916 + 0.0) = 0.0%

Вы должны заметить, что большая покупка кажется нереальной с вероятностью 0% только потому, что не было наблюдения, когда клиент, совершивший большую покупку, приходил один. Вы можете исправить это, сгладив данные, используя сглаживание по Лапласу.

**Сглаживание Лапласа** 

Сглаживание по методу Лапласа буквально добавляет единицу к каждой комбинации категории и категориальной переменной. Это помогает, поскольку предотвращает выбивание всего класса только из-за одной переменной.

|     Class       |     Visited Other Dept     |     Did Not Visit    Other Dept     |     Came Alone     |     Did Not Come Alone     |
|-----------------|----------------------------|-------------------------------------|--------------------|----------------------------|
|    Browser      |    200                     |    0                                |    75              |    125                     |
|    Easy Sale    |    150                     |    200                              |    250             |    100                     |
|    Big Sale     |    60                      |    60                               |    0               |    120                     |


Теперь вы можете видеть, что есть пара нулей. Мы заполняем эти пробелы, добавляя один к каждой ячейке в таблице.

| Class     | Visited Other Dept | Did Not Visit Other Dept | Came Alone | Did Not Come Alone |
|-----------|--------------------|--------------------------|------------|--------------------|
| Browser   | 201                | 1                        | 76         | 126                |
| Easy Sale | 151                | 201                      | 251        | 101                |
| Big Sale  | 61                 | 61                       | 1          | 121                |

Поскольку мы добавляем одну ячейку ко всем ячейкам, пропорции практически одинаковы. Чем больше у вас данных, тем меньше влияние будет добавлено на вашу модель. Непрерывные (числовые) значения в наивном байесовском У вас есть два варианта. 

1. «Дискретизируйте» переменные, разбивая / объединяя числовые значения в диапазоны. 
2. Используйте функцию Гаусса, чтобы определить вероятность непрерывного значения. 

Делать непрерывные переменные дискретными, например, превращение возраста в ребенка, взрослого и старшего просто создает новые переменные и различные значения. Это, наверное, самая простая вещь, которую нужно сделать. 

Во-вторых, вы можете преобразовать свои непрерывные переменные в вероятность при нормальном распределении.

** * Практика наивного Байеса * **

```{r}
library(e1071)
set.seed(1)
no_resp <- 500
resp <- 100
response <- factor(c(rep(0,no_resp),rep(1,resp)))
purchased_previously <- factor(c(sample(0:1,no_resp,prob=c(0.6,0.4),replace=T),
                          sample(0:1,resp,prob=c(0.2,0.8),replace=T)))
opened_previously <- factor(sample(0:1,(no_resp+resp),prob=c(0.8,0.2),replace = T))
sales_12mo <- c(rnorm(n=no_resp,mean = 50, sd = 10),
                rnorm(n=resp,mean = 60, sd = 5))
none_open_buy <- factor(c(sample(0:1, no_resp,prob=c(0.8,0.2),replace=T),
                          rep(1,resp)))
test_var <- sample(LETTERS[1:2],(resp+no_resp),replace=T)
 
naive_data <- data.frame(purchased_previously = purchased_previously,
                         opened_previously = opened_previously,
                         sales_12mo = sales_12mo,
                         none_open_buy = none_open_buy,
                         test_var = test_var,
                         response = response)
 
naive_data <- naive_data[sample(1:nrow(naive_data),nrow(naive_data)),]
 
train <- naive_data[1:(nrow(naive_data)*.7),]
test <- naive_data[(nrow(naive_data)*.7+1):nrow(naive_data),]

```

```{r}

nb_default <- naiveBayes(response~., data=train[,-4])
default_pred <- predict(nb_default, test, type="class")
 
table(default_pred, test$response,dnn=c("Prediction","Actual"))

```
Функция naiveBayes - это простая, элегантная реализация алгоритма наивного байесовского алгоритма. Есть только несколько параметров, которые вы должны учитывать

```{r eval=FALSE, include=FALSE}
naiveBayes(formula, data, laplace = 0, subset, na.action = na.pass)
```

 Формула традиционная Y ~ X1 + X2 +… + Xn Данные обычно являются датафреймами числовых или факторных переменных. Лаплас обеспечивает сглаживающий эффект, подмножество позволяет вам использовать только подмножество выбора ваших данных на основе некоторого логического фильтра na.action позволяет вам определить, что делать, если вы столкнулись с отсутствующим значением в наборе данных. Единственные параметры, которые у нас есть причина изменить в этом случае, это значение сглаживания по Лапласу. Поскольку простой алгоритм Байеса очень прост, нам не нужно тратить много времени на настройку наших параметров.

```{r}
nb_laplace1 <- naiveBayes(response~., data=train, laplace=1)
laplace1_pred <- predict(nb_laplace1, test, type="class")
 
table(laplace1_pred, test$response,dnn=c("Prediction","Actual"))
```
Функция naiveBayes включает параметр Лапласа. Какое бы положительное целое число оно ни было установлено, оно будет добавлено для каждого класса. Мы можем видеть, что условные вероятности для двух моделей теперь разные. Чем больше значение сглаживания Лапласа, тем больше вы делаете модели одинаковыми. 

```{r}
nb_default$tables$opened_previously
 
nb_laplace1$tables$opened_previously
```
**Ограничения на входной датасет**

Функция naiveBayes принимает числовые или факторные переменные во фрейме данных или числовой матрице. Важно отметить, что отдельные векторы не будут работать для входных данных, но будут работать для зависимой переменной (Y). Факторные переменные и символьные переменные принимаются. Символьные переменные приводятся в Факторы. Числовые переменные распределены нормально. Тогда числовая переменная будет преобразована в вероятность на этом распределении.


**Структура объекта модели naiveBayes**


*apriori* - это априорная вероятность для каждого класса в вашем тренировочном наборе. 
*levels* - это допустимые классы в вашей модели. 

Атрибут таблиц хранит условные вероятности для каждого фактора и комбинации классов. Мы можем легко рассчитать эти же таблицы, используя *table* и *prop.table*.
```{r}
prop.table(table(train$response,train$opened_previously, dnn=c("Response","Past Opened")))
            
nb_default$tables$opened_previously

```

Вы можете увидеть, насколько лучше использовать результаты naiveBayes, а не вычислять таблицы вручную.  Для непрерывных переменных вместо таблицы условной вероятности функция naiveBayes возвращает таблицу среднего и стандартного отклонения.

Эти вычисления среднего и стандартного отклонения обеспечивают нормальное распределение для каждого класса. Вы можете отразить то, что делает функция naiveBayes, используя pnorm (x, mean =, sd =) для каждого класса.

**Используем naiveBayes классификатор**

```{r}
default_pred <- predict(nb_default, test, type="class")


```

По умолчанию предикат возвращает класс с наибольшей вероятностью для этой прогнозируемой строки.

```{r}
default_raw_pred <- predict(nb_default, test, type="raw")
```

Функция предсказания позволяет вам указать, хотите ли вы выбрать наиболее вероятный класс или получить вероятность для каждого класса. Ничего не меняется, за исключением того, что для параметра type установлено значение «raw». В случае, если ваши данные плохо распределены по переменной класса, у R возникают проблемы с этим. Если у вас есть какой-либо класс только с одним экземпляром, модель naiveBayes все еще будет тренироваться, и прогнозирование наиболее вероятного класса также будет работать. Это тип = «raw», который потерпит неудачу.

### Кластеризация методом K-средних

K-Means - означает, что кластеризация использует «центроиды», K различных случайно инициированных точек в данных и присваивает каждую точку данных ближайшему центроиду. 
После того, как каждая точка была назначена, центроид перемещается к среднему значению всех назначенных ему точек. Затем процесс повторяется: каждая точка назначается ближайшему центроиду, центроиды перемещаются к среднему значению назначенных ему точек. Алгоритм выполняется, когда точка не изменяется, назначается центроид. Что такое K-означает кластеризацию Кластеризация, как правило, является методом «обучения без учителя». Это означает, что у нас нет целевой переменной. Мы просто позволяем шаблонам в данных стать более очевидными. Кластеризация K-средних отличается от иерархической, поскольку она создает K случайных центроидов, разбросанных по данным. Алгоритм выглядит немного как ... Инициализируйте K случайных центроидов. Вы можете выбрать K случайных точек данных и сделать их своими отправными точками. В противном случае вы выбираете K случайных значений для каждой переменной. Для каждой точки данных посмотрите, какой центроид является ближайшим к нему. Используя какое-то измерение, например, евклидово или косинусное расстояние. Назначьте точку данных ближайшему центроиду. Для каждого центроида переместите центроид к среднему значению точек, назначенных этому центроиду. Повторяйте последние три шага, пока назначение центроида больше не изменится. Говорят, что алгоритм «сходится», когда больше нет изменений. Эти центроиды действуют как среднее представление точек, которые ему назначены. Это дает вам историю почти сразу. Вы можете сравнить значения центроидов и сказать, предпочитает ли один кластер группу переменных или кластеры имеют логические группы ключевых переменных. Визуализация К-средних Кластеризация K-средних дает очень хороший визуальный элемент, поэтому здесь приведен краткий пример того, как может выглядеть каждый шаг.

![КЛАСТЕРИЗАЦИЯ](http://www.learnbymarketing.com/wp-content/uploads/2015/01/method-k-means-steps-initial.png)

![КЛАСТЕРИЗАЦИЯ](http://www.learnbymarketing.com/wp-content/uploads/2015/01/method-k-means-steps-example.png)

Итерация 2 показывает новое расположение центров центроидов. 
Итерация 3 имеет несколько синих точек при движении центроидов. Переходя к итерации 6, мы видим, что красный центроид переместился дальше вправо. 
Итерация 9 показывает, что зеленое сечение намного меньше, чем в итерации 2, синий цвет занял верх, а красный центроид тоньше, чем в итерации 6. 
Результаты 9-й итерации были такими же, как и результаты 8-й итерации, поэтому она «сошлась».



**На сколько кластеров нужно делить данные?**

Выбор начального количества кластеров SQRT (N / 2) - есть эмпирическое правило. Если у вас есть 100 точек данных, вы можете начать с SQRT (100/2) = SQRT (50) = 7 кластеров, а затем уменьшать.

Лучше всего алгоритм работает на числовых данных Поскольку алгоритм k-средних вычисляет расстояние между двумя точками, вы не можете сделать это с категориальными (низкими, средними, высокими) переменными. Простой обходной путь для нескольких категориальных переменных - это вычисление процента совпадений каждой переменной по сравнению с центроидом кластера. Измерение производительности K-средних Однородность и полнота. Если у вас есть уже существующие метки классов, которые вы пытаетесь дублировать с помощью кластеризации k-средних, вы можете использовать два показателя: однородность и полнота. Однородность означает, что все наблюдения с одинаковой меткой класса находятся в одном кластере. Полнота означает, что все члены одного класса находятся в одном кластере.  WithinSS - Сумма квадратов Inside измеряет сумму квадратов разности от центра кластера. Меньшее значение InsideSS (или SSW) означает, что в данных этого кластера меньше различий. Вот пример набора кластеров и их данных.

**Измерение производительности K-средних**

Однородность и полнота. Если у вас есть уже существующие метки классов, которые вы пытаетесь дублировать с помощью кластеризации k-средних, вы можете использовать два показателя: однородность и полнота. Однородность означает, что все наблюдения с одинаковой меткой класса находятся в одном кластере. Полнота означает, что все члены одного класса находятся в одном кластере. WithinSS - Сумма квадратов Inside измеряет сумму квадратов разности от центра кластера. Меньшее значение InsideSS (или SSW) означает, что в данных этого кластера меньше различий. 

Вот пример набора кластеров и их данных.


| Cluster | X    | Y    | Cluster VAR X | Cluster VAR Y | Var SUM |
|---------|------|------|---------------|---------------|---------|
| A       | 5    | 10   | 0.063         | 0.063         | 0.125   |
| A       | 7    | 12   | 3.063         | 5.063         | 8.125   |
| A       | 4    | 8    | 1.563         | 3.063         | 4.625   |
| A       | 5    | 9    | 0.063         | 0.563         | 0.625   |
| B       | 2    | 6    | 0.000         | 0.063         | 0.063   |
| B       | 1    | 4    | 1.000         | 3.063         | 4.063   |
| B       | 2    | 7    | 0.000         | 1.563         | 1.563   |
| B       | 3    | 6    | 1.000         | 0.063         | 1.063   |
| C       | 9    | 19   | 0.111         | 4.000         | 4.111   |
| C       | 9    | 21   | 0.111         | 0.000         | 0.111   |
| C       | 10   | 23   | 0.444         | 4.000         | 4.444   |
| Avg A   | 5.25 | 9.75 |               | SSW A         | 13.500  |
| Avg B   | 2    | 5.75 |               | SSW B         | 6.750   |
| Avg C   | 9.33 | 21   |               | SSW C         | 8.667   |

Кластер VAR X берет каждую строку и вычитает ее из среднего значения соответствующего кластера для X. То же самое касается кластера VAR Y. Var SUM складывает два столбца VAR кластера. Внизу SSW (A, B, C) складывает строки для своего соответствующего кластера. Мы видим, что WithinSS для кластера ... А = 13,5 B = 6,75 С = 8,667 ИТОГО WithinSS - это сумма этих трех значений. 13,5 + 6,47 + 8,667 = 28,637 TotalSS - это тот же процесс, игнорирующий кластерные группы. Для таблицы выше, общая сумма квадратов будет 536,18

** Кластеризация k-средних ПРАКТИКА**

Описание: функция kmeans () в R требует, как минимум, числовые данные и количество центров (или кластеров). Центры кластера вытягиваются с помощью *$center*. Назначения кластера извлекаются с помощью $ cluster. Вы можете оценить кластеры, посмотрев на $totss и $weenss. В R k-Means доступна по умолчанию в базовом пакете, *kmeans()*. Требуется только  матрица или датафрейм с только числовыми значениями и количество центров кластеров *k*.

```{r eval=FALSE, include=FALSE}
kmeans(x, centers, iter.max = 10, nstart = 1, algorithm = c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen"), trace=FALSE)
```

*Х* - это ваш фрейм данных или матрица. Все значения должны быть числовыми. Если у вас есть id в наборе данных, убдалите, или номер будет включен в расчет. Центры - это *K*. *centers* = 5 приведет к созданию 5 кластеров. Вы должны определить соответствующий номер для К. iter.max - количество повторений алгоритмом назначения кластера и перемещения центроидов. *nstart* - количество повторных выборок начальных точек.Сначала ищутся начальные точки, которые имеют наименьшее значение в пределах суммы квадратов (в пределах). Это означает, что он пробует выборки «nstart», назначает кластеры для каждой точки данных  *nstart* выбирает центры, которые имеют наименьшее расстояние от точек данных до центроидов. trace дает подробный вывод, показывающий ход выполнения алгоритма.


Готовая реализация K Means в R предлагает три алгоритма (Lloyd и Forgy - это один и тот же алгоритм, названный по-разному). По умолчанию используется алгоритм Хартиган-Вонга, который часто является самым быстрым. R предоставляет алгоритм Ллойда в качестве опции для kmeans. Как и алгоритм MacQueen (MacQueen, 1967),H-W обновляет центроиды при каждом перемещении точки; это также дает экономящий время результат при проверке ближайшего кластера. С другой стороны, алгоритм k-средних Ллойда является первым и самым простым из всех этих алгоритмов кластеризации. Алгоритм Ллойда (Lloyd, 1957) берет набор наблюденийи группирует их в k групп. Он пытается минимизировать сумму квадратов внутри кластера где u_i - среднее значение всех точек в кластере S_i.

```{r}
data <-read.csv("Wholesale customers data.csv",header=T)
summary(data)
```

Очевидно, что есть большая разница для лучших клиентов в каждой категории. Можно удалить эти выбросы, но точки зрения бизнеса, вам не нужен алгоритм кластеризации для определения того, что покупают ваши топовые клиенты. Вам обычно нужно кластеризация и сегментация для ваших средних 50%. С учетом вышесказанного, давайте попробуем удалить 5 лучших клиентов из каждой категории.

```{r}
topncusts <- function (data,cols,n=5) {
idxtoremove <-integer()
for (c in cols){
col.order <-order(data[,c],decreasing=T) 
idx <-head(col.order, n) 
idxtoremove <-union(idxtoremove,idx)
}
return(idxtoremove) 
}
top.custs <-topncusts(data,cols=3:8,5)
length(top.custs) 
data[top.custs,] 
data.rm.top<-data[-c(top.custs),] 
```

Теперь мы можем выполнить кластерный анализ после удаления id - Channel и Region. Это два поля идентификатора, которые не используются при кластеризации.

```{r}
set.seed(7)
k <-kmeans(data.rm.top[,-c(1,2)], centers=5) 
k$centers 
table(k$cluster)
```

Теперь мы можем начать интерпретировать результаты кластера: Кластер 1 выглядит как много бакалеи и моющих средств выше среднего, но с низким содержанием свежих продуктов. Кластер 3 является доминирующим в категории Fresh. Кластер 5 может быть либо универсальным кластером «мусорной корзины», либо представлять небольших клиентов. Измерение, которое является более относительным, будет внутри и между. k $ insidess скажет вам сумму квадрата расстояния от каждой точки данных до центра кластера. Видим большие значения  - значит есть выбросы или нужно создать больше кластеров. k$weenss показывает сумму квадратов расстояний между центрами кластеров. В идеале вы хотите, чтобы кластерные центры находились далеко друг от друга. Важно попробовать другие значения для K. Затем вы можете сравнить внутри и между. Это поможет вам выбрать наилучшее значение K. Например, с этим набором данных, что если вы запустили K с 2 по 20 и построили итоговое значение в виде суммы квадратов? Вы должны найти точку перегиба. Везде, где график изгибается и перестает приносить прибыль, вы называете это своим К.

```{r}
rng<-2:20 #K from 2 to 20
tries <-100 #Run the K Means algorithm 100 times
avg.totw.ss <-integer(length(rng)) #Set up an empty vector to hold all of points
for(v in rng){ # For each value of the range variable
 v.totw.ss <-integer(tries) #Set up an empty vector to hold the 100 tries
 for(i in 1:tries){
 k.temp <-kmeans(data.rm.top,centers=v) #Run kmeans
 v.totw.ss[i] <-k.temp$tot.withinss#Store the total withinss
 }
 avg.totw.ss[v-1] <-mean(v.totw.ss) #Average the 100 total withinss
}
plot(rng,avg.totw.ss,type="b", main="Total Within SS by Various K",
 ylab="Average Total Within Sum of Squares",
 xlab="Value of K")
```

### Случайный лес - random forest

Основная идея алгоритма - если мы сделаем очень много разных моделей с использованием некого алгоритма и усредним результат их предсказаний, то итоговый результат будет существенно лучше. Это, так называемое, обучение ансамбля в действии. Алгоритм Random Forest потому и называется Случайный Лес, для полученных данных он создает множество деревьев приятия решений и потом усредняет результат их предсказаний. Важным моментом тут является элемент случайности в создании каждого дерева. Ведь понятно, что если мы создадим много одинаковых деревьев, то результат их усреднения будет обладать точностью одного дерева.

Предположим, у нас есть некие данные на входе. Каждая колонка соответствует некоторому параметру, каждая строка соответствует некоторому элементу данных.
![матрица](http://3.bp.blogspot.com/-pJmWokzLCOA/T4RRstQ21aI/AAAAAAAABy4/3G-kBgXiXSY/s320/RF1.png)


Мы можем выбрать, случайным образом, из всего набора данных некоторое количество столбцов и строк и построить по ним дерево принятия решений.

![Древо](http://1.bp.blogspot.com/-B4JrdL70teo/T4RRxwQj6sI/AAAAAAAABzE/qBqKb3ykUtY/s1600/RF2.png)

Дальше мы можем повторить эту процедуру много-много раз и получить множество различных деревьев. Алгоритм построения дерева очень быстр. И поэтому нам не составит большого труда сделать столько деревьев, сколько будет нужно. При этом, все эти деревья в некотором смысле случайны, ведь для создания каждого из них мы выбирали случайное подмножество данных.

![Лес](http://1.bp.blogspot.com/-7PQw0YCbdw8/T4RR2NvmoPI/AAAAAAAABzQ/mH1U6M05GNY/s320/RF3.png)


В деревьях принятия решений выделение случайного подмножество происходит на каждом шаге построения дерева. Но это не меняет сути и результаты получаются сравнимыми.

Этот удивительно простой алгоритм и самым сложным шагом в его реализации является построение дерева decision tree. И не смотря на свою простоту он дает очень хорошие результаты в реальных задачах. С практической точки зрения у него есть одно огромное преимущество: он почти не требует конфигурации. Если мы возьмем любой другой алгоритм машинного обучения, будь то регрессия или нейронная сеть, они все имеют кучу параметров и их надо уметь подбирать под конкретную задачу. Алгоритм random forest имеет один параметр: размер случайного подмножества выбираемого на каждом шаге построения дерева. Этот параметр важен, но даже значения по умолчанию позволяют получить приемлимые результаты.
```{r eval=FALSE, include=FALSE}
install.packages("rusquant", repos="http://R-Forge.R-project.org")
```
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(rusquant)
getSymbols('GAZP', from='2003-05-01', to='2012-05-09', src='Finam')

label <- (Vo(GAZP) - lag(Vo(GAZP)))/lag(Vo(GAZP))
names(label) <- c('label')
data <- lag(GAZP)

data <-cbind(lag(label), lag(label, k=2), lag(label, k=3), lag(label, k=4), lag(label, k=5), RSI(Cl(data)), MACD(Cl(data)), volatility(data), factor(weekdays(index(label))))
names(data) <- c('Volume1', 'Volume2', 'Volume3', 'Volume4', 'Volume5', 'RSI', 'macd', 'signal', 'volatility', 'weekdays')
data <- cbind(label, data)
data <- data[complete.cases(data),]

split <- runif(dim(data)[1]) > 0.2
train <- data[split,]
test <- data[!split,]


```
```{r}

library(randomForest)
rf <- randomForest(label ~ ., train)
predictions <- predict(rf, test)
print(sqrt(sum((as.vector(predictions - test$label))^2))/length(predictions))
```
Можно ли как-то улучшить этот алгоритм? Если не считать вопросы скорости работы, есть только один параметр который имеет смысл менять mtry - он определяет, сколько столбцов тестируется при поиске следующего разбиения, для каждого дерева. Но его изменения не сказываются на результате принципиальным образом. Подобрать оптимальное значение mtry поможет функция tuneRF Кроме того, random forest позволяет оценить качество используемых предикторов.
```{r}
importance(rf)
```

Т.е. самым значимым предиктором оказалось изменение объема в предыдущий день, все остальные несли гораздо меньше информации. Это функция алгоритма random forest может помочь при выборе предикторов и провести простейший Т.е. самым значимым предиктором оказалось изменение объема в предыдущий день, все остальные несли гораздо меньше информации. Это функция алгоритма random forest может помочь при выборе предикторов и провести простейший feature selection.

### Логит регрессия

Логистическая регрессия используется для прогнозирования двоичного результата (1/0, Да/Нет, True/False) для заданного набора независимых переменных. Чтобы представить двоичный/категориальный результат, мы используем фиктивные переменные. Вы можете также думать о логистической регрессии как о специальном случае линейной регрессии, когда результат в  факторной форме, и мы используем логарифм коэффициентов в качестве зависимой переменной. Говоря простыми словами, она прогнозирует вероятность появления события путем подбора данных для функции logit. 
  
**Вывод уравнения логистической регрессии**
  
Логистическая регрессия является частью более широкого класса алгоритмов, известных как обобщенная линейная модель (glm).Эта модель была предложена для обеспечения возможности использования линейной регрессии в решении проблем, которые нельзя было напрямую решить с помощью линейной регрессии.

Фундаментальное уравнение обобщенной линейной модели имеет вид: 
  
g(E(y)) = α + βx1 + γx2 
  
Здесь g () - функция связи, E (y) - ожидаемое значение целевой переменной, а α + βx1 + γx2 - линейный предиктор (α, β, γ прогнозируются). Роль функции связи заключается в том, чтобы «связать» ожидаемые значения y с линейным предиктором. 
  
**Важные моменты**
  
GLM не предполагает линейной зависимости между зависимыми и независимыми переменными. Однако она предполагает линейную зависимость между функцией связи и независимыми переменными в модели logit. 
Зависимая переменная не обязательно должна быть нормально распределенной. 

```{r}
lgt<-read.csv("https://raw.githubusercontent.com/Peaceful-learner/Dressify-Challenge/master/train.csv")
lgt<-lgt[,-c(16:19)]
split <- runif(dim(lgt)[1]) > 0.2
dresstrain <- lgt[split,]
dresstest<- lgt[!split,]
```

```{r}
model<-glm (Recommended ~ .-ID, data = dresstrain, family = binomial) 
summary(model) 
```
```{r}
predict1<-predict(model, type = 'response') 
  

table(dresstrain$Recommended, predict1 > 0.5) 
  
library(ROCR) 
ROCRpred <- prediction(predict1, dresstrain$Recommended) 
ROCRperf <- performance(ROCRpred, 'tpr','fpr') 
plot(ROCRperf, colorize = TRUE, text.adj = c(-0.2,1.7)) 
 
library(ggplot2) 
ggplot(dresstrain, aes(x=Rating, y=Recommended)) + geom_point() +  
stat_smooth(method="glm", se=FALSE) 
```

### Задание 

Выбрать датасет из UCI репо и провести классификацию случайным лесом и логистической регрессией и сравнить результаты

```{r}

```

### Домашнее задание - до 29.11

*Выбрать данные кластеризации. Найти оптимальное число кластеров. Попробовать провести кластерный разными методами*
